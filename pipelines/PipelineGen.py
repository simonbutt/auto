from langchain.chat_models import ChatOpenAI
from langchain.prompts.chat import (
    ChatPromptTemplate,
    SystemMessagePromptTemplate,
    AIMessagePromptTemplate,
    HumanMessagePromptTemplate,
)
from langchain.schema import AIMessage, HumanMessage, SystemMessage
import json
import prompts.pipeline as pipeline_prompt
import prompts.component as component_prompt
import logging


class PipelineGen:

    """
    PipelineGen is a class that contains the methods to generate Vertex pipeline components based on the user's input.

    PipelineGen uses the OpenAI GPT-3 API to generate the code.
    It will then attempt to deploy the code to the user's Google Cloud environment and learn from failed pipeline runs.
    """

    def __init__(self, model_name: str, temperature: float = 0.2) -> None:
        self.system_context = f"{component_prompt.COMPONENT_SYSTEM_FULL_CONTEXT}\n{component_prompt.COMPONENT_TEMPLATE_PROMPT}"
        self.pipeline_prompt = pipeline_prompt.PIPELINE_TEMPLATE_PROMPT
        self.chat = ChatOpenAI(model_name=model_name, temperature=temperature)

    def _get_code(self, generated_text: dict) -> str:
        response_code_list = generated_text.split("```")
        if len(response_code_list) > 1:
            return response_code_list[1].removeprefix("\n")
        else:
            logging.warning(
                "Generated response snippet doesn't contain ``` code block, highly likely to be incorrect"
            )
            return response_code_list[0]

    def generate_pipeline(
        self, component_name, environment_variables: list = []
    ) -> str:
        """
        Generates a pipeline that includes the component code generated by generate_component

        Limitation: Currently only allows for a single component. TODO: Allow for multiple components and DAG generation.
        """
        with open(f"./lib/component/{component_name[0]}.py", "r") as f:
            component_code = f.read()

        component_message_chain = [
            SystemMessage(content=self.system_context),
            HumanMessage(content=f"Write a component that {component_name}"),
            AIMessage(content=component_code),
            HumanMessage(content=self.pipeline_prompt),
        ]
        ai_pipeline_response = self.chat(
            component_message_chain,
        )
        return self._get_code(ai_pipeline_response.content)

    def write_to_file(self, pipeline_code: str) -> None:
        """
        Writes the component code to a .py file
        """
        pipeline_name = pipeline_code.split("def ")[1].split("(")[0]

        with open(f"./lib/pipeline/{pipeline_name}.py", "w") as f:
            f.write(pipeline_code)


if __name__ == "__main__":
    import os

    log_dict = {
        "debug": logging.DEBUG,
        "info": logging.INFO,
        "warning": logging.WARNING,
        "error": logging.ERROR,
    }

    # Log Level default to INFO
    logging.basicConfig(
        level=log_dict.get(os.getenv("LOG_LEVEL", "INFO").lower(), "INFO")
    )
    model_name: str = os.getenv("MODEL_NAME")

    pgen = PipelineGen(
        system_prefix=prompt.SYSTEM_PREFIX,
        template_prompt=prompt.PIPELINE_TEMPLATE_PROMPT,
        model_name=model_name,
    )

    pgen.generate_pipeline(component_name="add")
